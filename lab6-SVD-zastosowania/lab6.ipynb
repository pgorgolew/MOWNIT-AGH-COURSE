{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Metody Obliczeniowe w Nauce i Technice Laboratorium 6\n",
    "## Article searcher\n",
    "### Paweł Gorgolewski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pawel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "import wikipedia as wiki\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from typing import List, Dict\n",
    "from wikipedia.exceptions import WikipediaException\n",
    "from collections import namedtuple, defaultdict\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy import sparse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Generowanie artykułów z wikipedii\n",
    "Dump indeksów artykułów z wikipedii został pobrany poprzez link `https://dumps.wikimedia.org/enwiki/20220401/`\n",
    "\n",
    "Wykorzystujac pobrany dump, zaciągamy zawartość tytułu poprzez bilbiotekę `wikipedia`. Załadowany w ten sposób artykuł, zapisywany jest do folderu `data` z wykorzystaniem pythonowego `pickla`\n",
    "Aby pobrać arykuły, należy uruchomić poniższy blok z odkomentowanymi dwoma ostatnimi liniami."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def get_first_n_titles(n=3500, file='data\\wiki-pages-indexes.txt'):\n",
    "    titles = []\n",
    "    with open(file, \"r\", encoding='utf8') as f:\n",
    "        csv_reader = csv.reader(f, delimiter=\":\")\n",
    "        for row in csv_reader:\n",
    "            if n < 0:\n",
    "                break\n",
    "\n",
    "            title = row[-1]\n",
    "            if '/' in title:\n",
    "                continue\n",
    "\n",
    "            titles.append(title)\n",
    "            n-=1\n",
    "\n",
    "    random.shuffle(titles)\n",
    "    return titles\n",
    "\n",
    "def get_articles_content_and_save_pickle(titles: List[str]):\n",
    "    not_matched = 0\n",
    "    for title in titles:\n",
    "        try:\n",
    "            wiki_page = wiki.page(title)\n",
    "            wiki_content = wiki_page.content\n",
    "            wiki_url = wiki_page.url\n",
    "            title_dict = {'content': wiki_content, 'url': wiki_url}\n",
    "            pickle.dump(title_dict, open(f'data\\\\{title}', \"wb\"))\n",
    "        except WikipediaException:\n",
    "            not_matched += 1\n",
    "        except Exception as ex:\n",
    "            print(f\"EXCEPTION CAUGHT!\\n{str(ex)}\")\n",
    "\n",
    "    print(f\"Not matched {not_matched} titles\")\n",
    "\n",
    "#titles_to_load = get_first_n_titles()\n",
    "#get_articles_content_and_save_pickle(titles_to_load)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tworzenie słownika z nazwą artykułu jako klucz oraz tekstem jako wartość\n",
    "Poniższy kod zapisuje stworzoną strukturę do ścieżki `.\\data\\dumps`. Jest także możliwość wczytania zapisanej wcześniej struktry przy użyciu funkjci `get_articles_from_dump`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def get_articles(to_omit_file='wiki-pages-indexes.txt'):\n",
    "    data_by_title = dict()\n",
    "    for title in pickles():\n",
    "        if title == to_omit_file or \"dump\" in title:\n",
    "            continue\n",
    "\n",
    "        data_by_title[title] = pickle.load(open(f\"data\\\\{title}\", \"rb\"))\n",
    "\n",
    "    pickle.dump(data_by_title, open(f\"data\\\\dumps\\\\articles_dict_dump_of_len_{len(data_by_title)}\", \"wb\"))\n",
    "    return data_by_title\n",
    "\n",
    "def pickles(path='data'):\n",
    "    for content in os.listdir(path):\n",
    "        if os.path.isfile(os.path.join(path, content)):\n",
    "            yield content\n",
    "\n",
    "def get_articles_from_dump(dirpath=\"data\\\\dumps\\\\\", dump_name=\"articles_dict_dump_of_len_2890\"):\n",
    "    path = os.path.join(dirpath, dump_name)\n",
    "    return pickle.load(open(path, \"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aby stworzyć nowy słownik, należy odkomentowac pierwszą i zakomentować drugą linijkę poniższego bloku"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "#loaded_articles = get_articles()\n",
    "loaded_articles = get_articles_from_dump()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ArticleParser\n",
    "Poniższa klasa wykonuje wszystkie czynności potrzebne do późniejszego wyszukiwania artykułów poprzez metodę `parse_artciles_and_prepare_term_by_document`. Wykonuje ona następujące czynności:\n",
    "1. przetwarza teksty artykułów\n",
    "2. tworzy `bags_of_words`\n",
    "3. tworzy rzadka macierz wektorów cech term-by-document\n",
    "4. przetwarza macierz wektorów cech używając IDF\n",
    "5. normalizuje wektory z macierzy cech\n",
    "6. tworzy i zapisuje nową macierz przy użyciu svd (domyślne k to 1200)\n",
    "\n",
    "Po jej wykonaniu, należy użyć metody `find_articles`, która wypisze najbardziej trafne artykuły. Aby usunąć szumy, należy podać argument `Ak_matrix=True`, dzięki czemu algorytm wykona się na macierzy Ak (domyślne `k` do svd to 200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "ContentData = namedtuple('ContentData', ['words_data', 'words_count'])\n",
    "class ArticlesParser:\n",
    "    def __init__(self, articles: Dict[str, Dict[str,str]]):\n",
    "        self.articles = articles\n",
    "        self.parsed_articles = dict()\n",
    "        self.ids_by_unique_word = None      # Dict[word: str, id: int]\n",
    "        self.term_by_document = None        # sparse_matrix\n",
    "        self.all_unique_words = None        # List[str]\n",
    "        self.all_articles_titles = None     # List[str]\n",
    "        self.all_words_data = defaultdict(lambda: 0)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.Ak_matrix = None\n",
    "\n",
    "    def parse_artciles_and_prepare_term_by_document(self):\n",
    "        print(\"STARTED\")\n",
    "        self.parse_articles()\n",
    "        print(\"AFTER ARTICLE PARSING\")\n",
    "        self.create_bags_of_words()\n",
    "        print(\"AFTER CREATING BAGS\")\n",
    "        self.create_term_by_document_matrix()\n",
    "        print(\"AFTER CREATING DOCUMENT BY TERM\")\n",
    "        self.multiply_term_by_document_by_IDF()\n",
    "        print(\"AFTER MULTIPLYING BY IDF\")\n",
    "        self.normalize_vectors()\n",
    "        print(\"AFTER NORMALIZATION\")\n",
    "        self.get_Ak_from_term_by_document()\n",
    "        print(\"AFTER ALL\")\n",
    "\n",
    "    def normalize_vectors(self):\n",
    "        for i in range(len(self.all_articles_titles)):\n",
    "            vector = self.term_by_document.getcol(i)\n",
    "            vector_norm = self.get_norm_from_vector(vector)\n",
    "            self.term_by_document[:, i] /= vector_norm\n",
    "\n",
    "    def parse_content(self, content: str, is_article=True):\n",
    "        content = content.lower()\n",
    "        content = re.sub(r'[^\\w\\s]','', content)\n",
    "        content = re.sub('[0-9]','', content)\n",
    "        content = re.sub(' {2} +',' ', content)\n",
    "        content = remove_stopwords(content)\n",
    "\n",
    "        words_data = defaultdict(lambda: 0)\n",
    "        words_count = 0\n",
    "        for word in content.split():\n",
    "            lemmatized_word = self.lemmatizer.lemmatize(word, pos='v')\n",
    "            words_data[lemmatized_word] += 1\n",
    "            if is_article:\n",
    "                self.all_words_data[lemmatized_word] += 1\n",
    "            words_count+=1\n",
    "\n",
    "        return ContentData(words_data=words_data, words_count=words_count)\n",
    "\n",
    "    def parse_articles(self):\n",
    "        self.all_articles_titles = list(self.articles.keys())\n",
    "        for article in self.all_articles_titles:\n",
    "            self.parsed_articles[article] = dict()\n",
    "            self.parsed_articles[article]['content_data'] = self.parse_content(self.articles[article]['content'])\n",
    "\n",
    "        self.all_unique_words = list(self.all_words_data.keys())\n",
    "        self.ids_by_unique_word = {self.all_unique_words[i]: i for i in range(len(self.all_unique_words))}\n",
    "\n",
    "    def create_bags_of_words(self):\n",
    "        for article in self.all_articles_titles:\n",
    "            self.parsed_articles[article]['bag_of_words'] = \\\n",
    "                self.create_bag_of_words(self.parsed_articles[article]['content_data'])\n",
    "\n",
    "    def create_bag_of_words(self, content_data):\n",
    "        article_unique_words = content_data.words_data.keys()\n",
    "        vector = sparse.dok_matrix(np.zeros((len(self.ids_by_unique_word), 1)))\n",
    "        for word in article_unique_words: #TODO maybe jakiś numpy mapping or coś\n",
    "            vector[self.ids_by_unique_word[word], 0] = content_data.words_data[word]\n",
    "\n",
    "        vector /= content_data.words_count\n",
    "        return sparse.csr_matrix(vector)\n",
    "\n",
    "    def create_term_by_document_matrix(self):\n",
    "        amount_of_articles = len(self.all_articles_titles)\n",
    "        amount_of_words = len(self.all_unique_words)\n",
    "        self.term_by_document = sparse.lil_matrix((amount_of_words, amount_of_articles))\n",
    "\n",
    "        for i in range(amount_of_articles):\n",
    "            self.term_by_document[:,i] = self.parsed_articles[self.all_articles_titles[i]]['bag_of_words']\n",
    "\n",
    "    def multiply_term_by_document_by_IDF(self):\n",
    "        articles_count = len(self.all_articles_titles)\n",
    "        self.term_by_document = sparse.csr_matrix(self.term_by_document)\n",
    "        for word in self.all_unique_words:\n",
    "            articles_with_word = self.calculate_articles_with_word(word)\n",
    "            idf = math.log(articles_count / articles_with_word)\n",
    "            id_of_word = self.ids_by_unique_word[word]\n",
    "            self.term_by_document[id_of_word] *= idf\n",
    "\n",
    "\n",
    "    def calculate_articles_with_word(self, word: str):\n",
    "        return sum(1 for article in self.parsed_articles.values() if word in article['content_data'].words_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_norm_from_vector(vector):\n",
    "        return math.sqrt(vector.power(2).sum())\n",
    "        \n",
    "    def find_articles(self, query, artciles_num_to_return, Ak_matrix=False):\n",
    "        if Ak_matrix:\n",
    "            matrix = self.Ak_matrix\n",
    "        else:\n",
    "            matrix = self.term_by_document\n",
    "\n",
    "        query_words_data = self.parse_content(content=query, is_article=False)\n",
    "        vector = self.create_bag_of_words(query_words_data)\n",
    "        vector_norm =  self.get_norm_from_vector(vector)\n",
    "\n",
    "        probabilities = []\n",
    "        for i in range(len(self.all_articles_titles)):\n",
    "            article = matrix.getcol(i)\n",
    "            product = (vector.T @ article)[0,0] #just getting first val\n",
    "            divider = vector_norm * self.get_norm_from_vector(article)\n",
    "            document_cosinus = product / divider\n",
    "            probabilities.append((document_cosinus, i))\n",
    "\n",
    "        probabilities.sort(key=lambda t: t[0], reverse=True)\n",
    "\n",
    "        print(f\"Articles found for guery: {query}\")\n",
    "        for probability, index in probabilities[:artciles_num_to_return]:\n",
    "            article = self.all_articles_titles[index]\n",
    "            print(f\"\\tARTICLE: {article}\\t\\tPROBABILITY: {probability}\\t\\tURL: {self.articles[article]['url']}\")\n",
    "\n",
    "    def get_Ak_from_term_by_document(self, k=200):\n",
    "        u, s, vt = sparse.linalg.svds(self.term_by_document, k=k)\n",
    "        print(\"AFTER SVD\")\n",
    "        u = sparse.csr_matrix(u)\n",
    "        s = sparse.diags(s)\n",
    "        vt = sparse.csr_matrix(vt)\n",
    "        self.Ak_matrix = u @ s @ vt\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "parser = ArticlesParser(loaded_articles)\n",
    "\n",
    "# For creating everything use this:\n",
    "#parser.parse_artciles_and_prepare_term_by_document()\n",
    "\n",
    "# For loading dump structures use this:\n",
    "parser.parse_articles()\n",
    "parser.term_by_document = pickle.load(open(f\"data\\\\dumps\\\\parser_term_by_document_with_{len(parser.articles)}_articles\", \"rb\"))\n",
    "parser.Ak_matrix = pickle.load(open(f\"data\\\\dumps\\\\parser_Ak_matrix_with_{len(parser.articles)}_articles\", \"rb\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wyszukiwanie dokumentów"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles found for guery: Football\n",
      "\tARTICLE: AmericanFootball\t\tPROBABILITY: 0.5485938704141171\t\tURL: https://en.wikipedia.org/wiki/American_football\n",
      "\tARTICLE: Bocce\t\tPROBABILITY: 0.49761829342838676\t\tURL: https://en.wikipedia.org/wiki/Association_football\n",
      "\tARTICLE: Australian rules football\t\tPROBABILITY: 0.48009088490857627\t\tURL: https://en.wikipedia.org/wiki/Australian_rules_football\n",
      "\tARTICLE: Bert Bell\t\tPROBABILITY: 0.27241190170102136\t\tURL: https://en.wikipedia.org/wiki/Bert_Bell\n",
      "\tARTICLE: Bristol City F.C\t\tPROBABILITY: 0.23865321959646157\t\tURL: https://en.wikipedia.org/wiki/Bristol_City_F.C.\n"
     ]
    }
   ],
   "source": [
    "parser.find_articles(\"Football\", 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-38-cbe19e8c9c6a>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mparser\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind_articles\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Football\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m5\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-35-f9a12a3648fe>\u001B[0m in \u001B[0;36mfind_articles\u001B[1;34m(self, query, artciles_num_to_return, Ak_matrix)\u001B[0m\n\u001B[0;32m    111\u001B[0m         \u001B[0mprobabilities\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    112\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mall_articles_titles\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 113\u001B[1;33m             \u001B[0marticle\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmatrix\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetcol\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    114\u001B[0m             \u001B[0mproduct\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mvector\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mT\u001B[0m \u001B[1;33m@\u001B[0m \u001B[0marticle\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;31m#just getting first val\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    115\u001B[0m             \u001B[0mdivider\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvector_norm\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_norm_from_vector\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marticle\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\scipy\\sparse\\_csr.py\u001B[0m in \u001B[0;36mgetcol\u001B[1;34m(self, i)\u001B[0m\n\u001B[0;32m    272\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[1;33m<\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[0mN\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    273\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mIndexError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'index (%d) out of range'\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 274\u001B[1;33m         indptr, indices, data = get_csr_submatrix(\n\u001B[0m\u001B[0;32m    275\u001B[0m             M, N, self.indptr, self.indices, self.data, 0, M, i, i + 1)\n\u001B[0;32m    276\u001B[0m         return self.__class__((data, indices, indptr), shape=(M, 1),\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "parser.find_articles(\"Football\", 5, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zapisywanie wyliczonych struktur"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_structures():\n",
    "    pickle.dump(parser.Ak_matrix, open(f\"data\\\\dumps\\\\parser_Ak_matrix_with_{len(parser.articles)}_articles\", \"wb\"))\n",
    "    pickle.dump(parser.term_by_document, open(f\"data\\\\dumps\\\\parser_term_by_document_with_{len(parser.articles)}_articles\", \"wb\"))\n",
    "\n",
    "#save_structures()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wnioski:\n",
    "1) Należałoby poprawić implementację, ponieważ wszelkie instrukcje zajmują bardzo dużo czasu.\n",
    "2) Wyszkiwarka zdaje się zwracać odpowiednie wyniki"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}